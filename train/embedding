import keras
import pandas as pd
from config import config
from keras.preprocessing.text import Tokenizer

max_features = 300
maxlen = 300
embed_size = 100
max_length_inp, max_length_targ= 500, 50


def tokenize(lang, max_len):
    tokenizer = Tokenizer(filters='', lower=False, num_words=max_features)
    tokenizer.fit_on_texts(lang)
    # 110k 的词典
    word_index = tokenizer.word_index
    tensor = tokenizer.texts_to_sequences(lang)
    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=max_len)
    return tensor, tokenizer, word_index


def load_train():
    data = pd.read_csv(config.traindata_path)
    source = [str(m) for m in data['Input'].values.tolist()]
    target = [str(m) for m in data['Report'].values.tolist()]
    input_tensor, tokenizer1, word_index1 = tokenize(source, max_length_inp)
    target_tensor, tokenizer2, word_index2 = tokenize(target, max_length_targ)
    return input_tensor, target_tensor, word_index1, word_index2, tokenizer1, tokenizer2


if __name__ == "__main__":
    load_train()